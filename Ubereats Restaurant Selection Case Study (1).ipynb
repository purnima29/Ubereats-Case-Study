{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "\n",
    "import json\n",
    "import requests\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import difflib\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "import math\n",
    "import re\n",
    "from collections import Counter\n",
    "from geopy import distance\n",
    "from math import radians, cos, sin, asin, sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# filtering local authorirty ids for a region\n",
    "\n",
    "region_name='London'\n",
    "url = \"https://api.ratings.food.gov.uk/authorities\"\n",
    "#params = {}\n",
    "headers = {\n",
    "    \"x-api-version\": '2',\n",
    "#             \"accept\": \"application/json\",\n",
    "#             \"content-type\": \"application/json\",\n",
    "#             \"Accept-Language\": \"en-GB\"\n",
    "            }\n",
    "response = requests.get(url,headers=headers).json()\n",
    "\n",
    "authority_df = pd.DataFrame(columns=['local_authority_id','local_authority_id_code','name']) \n",
    "for authorities in response['authorities']:\n",
    "    if authorities['RegionName'] == region_name:\n",
    "        local_authority_id = authorities['LocalAuthorityId']\n",
    "        local_authority_id_code = authorities['LocalAuthorityIdCode']\n",
    "        name = authorities['Name']\n",
    "        authority_df = authority_df.append({'local_authority_id':local_authority_id,'local_authority_id_code':local_authority_id_code\n",
    "                    ,'name':name  },ignore_index=True)\n",
    "authority_ids = authority_df['local_authority_id'].astype('str').to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using API to pull all restaurants in London"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# business types available\n",
    "\n",
    "business_types_df = pd.DataFrame(columns=['business_type_id','business_type_name'])\n",
    "\n",
    "url = \"https://api.ratings.food.gov.uk/businessTypes\"\n",
    "headers = {\"x-api-version\": '2'}\n",
    "response = requests.get(url,headers=headers).json()\n",
    "\n",
    "for business_types in response['businessTypes']:\n",
    "    business_type_id = business_types['BusinessTypeId']\n",
    "    business_type_name = business_types['BusinessTypeName']\n",
    "    business_types_df = business_types_df.append({'business_type_id':business_type_id,'business_type_name':business_type_name\n",
    "                           },ignore_index=True)\n",
    "\n",
    "# selecting business ids relevant to ubereats\n",
    "\n",
    "select_business_ids = ['7846','7841','7843','1','7844']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pulling dataset based on business type ids and authority ids\n",
    "\n",
    "df = pd.DataFrame(columns=['establishment_id','establishment_name'\n",
    "                            ,'establishment_address_line1','establishment_address_line2'\n",
    "                            ,'establishment_address_line3','establishment_address_line4'\n",
    "                            ,'establishment_lng','establishment_lat'\n",
    "                            ,'establishment_business_type','establishment_business_type_id'\n",
    "                            ,'authority_id_code']) \n",
    "for authority_id,business_id in itertools.product(authority_ids,select_business_ids):\n",
    "    url = \"https://api.ratings.food.gov.uk/establishments?businessTypeId=\"+business_id+\"&localAuthorityId=\"+authority_id+\"\"\n",
    "    headers = {\"x-api-version\": '2'}\n",
    "    response = requests.get(url,headers=headers).json()\n",
    "\n",
    "    time.sleep(1) #giving it a second before starting the for loop\n",
    "\n",
    "    for establishments in response['establishments']:\n",
    "        establishment_id = establishments['LocalAuthorityBusinessID']\n",
    "        establishment_name = establishments['BusinessName']\n",
    "        establishment_address_line1 = establishments['AddressLine1']\n",
    "        establishment_address_line2 = establishments['AddressLine2']\n",
    "        establishment_address_line3 = establishments['AddressLine3']\n",
    "        establishment_address_line4 = establishments['AddressLine4']\n",
    "        establishment_lng = establishments['geocode']['longitude']\n",
    "        establishment_lat = establishments['geocode']['latitude']\n",
    "        establishment_business_type = establishments['BusinessType']\n",
    "        establishment_business_type_id = establishments['BusinessTypeID']\n",
    "        authority_id_code = establishments['LocalAuthorityCode']\n",
    "        df = df.append({'establishment_id':establishment_id,'establishment_name':establishment_name\n",
    "                    ,'establishment_address_line1':establishment_address_line1,'establishment_address_line2':establishment_address_line2\n",
    "                    ,'establishment_address_line3':establishment_address_line3,'establishment_address_line4':establishment_address_line4\n",
    "                    ,'establishment_lng':establishment_lng,'establishment_lat':establishment_lat\n",
    "                    ,'establishment_business_type':establishment_business_type,'establishment_business_type_id':establishment_business_type_id\n",
    "                    ,'authority_id_code':authority_id_code\n",
    "                   },ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning and formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the data pulled using API to CSV\n",
    "\n",
    "df.to_csv('all_food_establishments_in_london.csv')\n",
    "\n",
    "# read data from CSV\n",
    "\n",
    "all_food_establishments_in_london = pd.read_csv('all_food_establishments_in_london.csv')\n",
    "\n",
    "# formatting data as required\n",
    "\n",
    "df_all = pd.read_csv('df_based_on_ids.csv')\n",
    "df_all = df_all.drop(columns=['Unnamed: 0'])\n",
    "\n",
    "# Removing duplicates and missing value treatment\n",
    "\n",
    "df_all = df_all.drop_duplicates()\n",
    "\n",
    "# Only comparing restaurants where GPS details are available\n",
    "\n",
    "df_all = df_all[df_all['establishment_lng'].notna()]\n",
    "df_all = df_all[df_all['establishment_lat'].notna()]\n",
    "\n",
    "# Concatenating Restaurant name and address\n",
    "\n",
    "df_all = df_all.fillna('')\n",
    "df_all['full_name'] = df_all['establishment_name'] + \" \" + df_all['establishment_address_line1'] + \" \" + df_all['establishment_address_line2'] + \" \" + df_all['establishment_address_line3'] + \" \" + df_all['establishment_address_line4']\n",
    "df_all\n",
    "\n",
    "# Reset index\n",
    "\n",
    "df_all.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read uber data from CSV\n",
    "\n",
    "df_uber = pd.read_csv(\"ubereats.csv\")\n",
    "\n",
    "#formatting data as required\n",
    "\n",
    "# Removing duplicates and missing value treatment\n",
    "\n",
    "df_uber = df_uber.drop_duplicates()\n",
    "df_uber = df_uber.dropna()\n",
    "\n",
    "# Removing Cuisine Prefix from Restaurant names\n",
    "\n",
    "df_uber['first_cuisine'] = df_uber.cuisine.str.split(',').str.get(0)\n",
    "df_uber['new_name'] = df_uber.apply(lambda row : str(row['name']).replace(str(row['first_cuisine']),''),axis=1)\n",
    "\n",
    "# Concatenating Restaurant name and address\n",
    "\n",
    "df_uber['full_name'] = df_uber['new_name'] + \" \" + df_uber['address']\n",
    "\n",
    "# Reset index\n",
    "\n",
    "df_uber.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matching Algortihm to find closest matching restaurant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dba58059949948db89190b53d7dd5448",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=13486.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "WORD = re.compile(r\"\\w+\")\n",
    "\n",
    "# function to convert text to vector\n",
    "\n",
    "def text_to_vector(text):\n",
    "    words = WORD.findall(text)\n",
    "    return Counter(words)\n",
    "\n",
    "# function to get cosine distance between set of strings\n",
    "\n",
    "def get_cosine(vec1, vec2):\n",
    "    intersection = set(vec1.keys()) & set(vec2.keys())\n",
    "    numerator = sum([vec1[x] * vec2[x] for x in intersection])\n",
    "\n",
    "    sum1 = sum([vec1[x] ** 2 for x in list(vec1.keys())])\n",
    "    sum2 = sum([vec2[x] ** 2 for x in list(vec2.keys())])\n",
    "    denominator = math.sqrt(sum1) * math.sqrt(sum2)\n",
    "\n",
    "    if not denominator:\n",
    "        return 0.0\n",
    "    else:\n",
    "        return float(numerator) / denominator\n",
    "    \n",
    "# function to get haversine distance between two points\n",
    "\n",
    "def haversine(lon1, lat1, lon2, lat2):\n",
    "    \"\"\"\n",
    "    Calculate the great circle distance between two points \n",
    "    on the earth (specified in decimal degrees)\n",
    "    \"\"\"\n",
    "\n",
    "    # convert decimal degrees to radians \n",
    "    lon1, lat1, lon2, lat2 = map(radians, [lon1, lat1, lon2, lat2])\n",
    "\n",
    "    # haversine formula \n",
    "    dlon = lon2 - lon1 \n",
    "    dlat = lat2 - lat1 \n",
    "    a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2\n",
    "    c = 2 * asin(sqrt(a)) \n",
    "\n",
    "    # 6367 km is the radius of the Earth\n",
    "    km = 6367 * c\n",
    "    return km\n",
    "\n",
    "''' matching algorithm that finds closest match for each uber restaurant in the \n",
    "universal dataset based on closest restaurant name and shortest distance based \n",
    "on latitude and longitude '''\n",
    "\n",
    "def get_score(orig_vect,comp_vect,lon1, lat1, lon2, lat2):\n",
    "    cosine = get_cosine(orig_vect, comp_vect)\n",
    "    dist = haversine(lon1, lat1, lon2, lat2)\n",
    "    \n",
    "    return (1.0000001 - cosine)**2 * dist\n",
    "\n",
    "def min_score_est(df_all,orig_vect,lon1, lat1):\n",
    "    score_list = []\n",
    "    \n",
    "    df_all['score'] = df_all.apply(lambda x: get_score(orig_vect,x['all_vector'],lon1, lat1,x['establishment_lng'],x['establishment_lat']),axis=1)\n",
    "\n",
    "    min_score = df_all['score'].min()\n",
    "    matching_rest = df_all[df_all['score'] ==  min_score]['establishment_name'].values[0]\n",
    "    matching_lat = df_all[df_all['score'] ==  min_score]['establishment_lat'].values[0]\n",
    "    matching_lng = df_all[df_all['score'] ==  min_score]['establishment_lng'].values[0]\n",
    "    return matching_rest,matching_lat,matching_lng,min_score\n",
    "\n",
    "# creating vectorized columns in both uber and universal dataset\n",
    "\n",
    "\n",
    "df_uber['uber_vector'] = df_uber['new_name'].astype(str).apply(text_to_vector)\n",
    "df_all['all_vector'] = df_all['establishment_name'].astype(str).apply(text_to_vector)\n",
    "\n",
    "\n",
    "df_uber['closest_match_restaurant'] = 0\n",
    "df_uber['closest_match_lat'] = 0\n",
    "df_uber['closest_match_lng'] = 0\n",
    "df_uber['score'] = 0\n",
    "score_list = []\n",
    "for index1 in tqdm_notebook(range(df_uber.shape[0])):\n",
    "    orig_vect = df_uber['uber_vector'].iloc[index1]\n",
    "#     p1 = df_uber['p1'].iloc[index1]\n",
    "    df_uber['closest_match_restaurant'].iloc[index1],df_uber['closest_match_lat'].iloc[index1],df_uber['closest_match_lng'].iloc[index1],df_uber['score'].iloc[index1] = min_score_est(df_all,orig_vect,df_uber['longitude'].iloc[index1], df_uber['latitude'].iloc[index1])\n",
    "    \n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final overall dataset with common restaurants removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_uber.to_csv('uber_restaurant_with_closest_match_1.csv')\n",
    "\n",
    "# Setting threshold on score to ensure accuracy\n",
    "\n",
    "df_uber_match_list = df_uber[df_uber['score']<=0.025]\n",
    "\n",
    "# Removing resturants found in uber dataset\n",
    "\n",
    "list_of_restaurants_not_on_uber = df_all.merge(df_uber, left_on=['establishment_name','establishment_lng','establishment_lat'], right_on=['closest_match_restaurant','closest_match_lng','closest_match_lat'], how='left', indicator=True)\n",
    "\n",
    "list_of_restaurants_not_on_uber = list_of_restaurants_not_on_uber[list_of_restaurants_not_on_uber['_merge'] == 'left_only']\n",
    "\n",
    "list_of_restaurants_not_on_uber.to_csv('list_of_restaurants_not_on_uber.csv')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 (General DS)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
